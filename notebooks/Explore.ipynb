{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import os\n",
    "from common.interfaces import RetrievedFile, FileDisplayLevel\n",
    "from retrieval.gold import GoldenRetriever\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "\n",
    "config = tomllib.load(open(\"configs/main.toml\", \"rb\"))\n",
    "working_stage = config[\"working_stage\"]\n",
    "dataset = \"princeton-nlp/SWE-bench_Lite\"\n",
    "split = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./working_stage/gold_retrieved_princeton-nlp__SWE-bench_Lite_dev.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20/23'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_stats_df(dataset, split, filter_by_count=None, code_only=True):\n",
    "    retriever = GoldenRetriever(dataset_name=dataset, split=split, num_shards=None, shard_id=None)\n",
    "    stats = retriever.collect_stats(code_only=True)\n",
    "    instance_ids = [k for k in stats['num_files'].keys()]\n",
    "    num_files = [v for v in stats['num_files'].values()]\n",
    "    num_edits = [v for v in stats['num_edits'].values()]\n",
    "    num_lines = [v for v in stats['num_lines'].values()]\n",
    "    num_imports = [v for v in stats['num_imports'].values()]\n",
    "\n",
    "    data = {\n",
    "        'instance_ids': instance_ids,\n",
    "        'num_files': num_files,\n",
    "        'num_edits': num_edits,\n",
    "        'num_imports': num_imports,\n",
    "        'num_lines': num_lines\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    if filter_by_count is not None:\n",
    "        df = df[df['num_files'] == filter_by_count]\n",
    "    return df\n",
    "\n",
    "df = make_stats_df(dataset, split, filter_by_count=None, code_only=True)\n",
    "simple_df = df[(df['num_files'] == 1) & (df['num_edits'] <= 2)]\n",
    "f\"{len(simple_df)}/{len(df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d.vega-embed details,\n",
       "  #altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_files\", \"title\": \"Number of Files\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Files per Instance\", \"width\": 400}, {\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_edits\", \"title\": \"Number of Edits\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Edits per Instance\", \"width\": 400}, {\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_lines\", \"title\": \"Number of Lines\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Lines per Instance\", \"width\": 400}, {\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_imports\", \"title\": \"Number of Imports\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Imports per Instance\", \"width\": 400}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-5c1dadf12b6504d095f5a414c8b1d6c6\": [{\"instance_ids\": \"sqlfluff__sqlfluff-1625\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 2}, {\"instance_ids\": \"sqlfluff__sqlfluff-2419\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 6}, {\"instance_ids\": \"sqlfluff__sqlfluff-1733\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 4}, {\"instance_ids\": \"sqlfluff__sqlfluff-1517\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 1, \"num_lines\": 11}, {\"instance_ids\": \"sqlfluff__sqlfluff-1763\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 2, \"num_lines\": 25}, {\"instance_ids\": \"marshmallow-code__marshmallow-1359\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 2}, {\"instance_ids\": \"marshmallow-code__marshmallow-1343\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 4}, {\"instance_ids\": \"pvlib__pvlib-python-1707\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 21}, {\"instance_ids\": \"pvlib__pvlib-python-1072\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 5}, {\"instance_ids\": \"pvlib__pvlib-python-1606\", \"num_files\": 1, \"num_edits\": 3, \"num_imports\": 0, \"num_lines\": 36}, {\"instance_ids\": \"pvlib__pvlib-python-1854\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 11}, {\"instance_ids\": \"pvlib__pvlib-python-1154\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 5}, {\"instance_ids\": \"pylint-dev__astroid-1978\", \"num_files\": 1, \"num_edits\": 3, \"num_imports\": 3, \"num_lines\": 27}, {\"instance_ids\": \"pylint-dev__astroid-1333\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 8}, {\"instance_ids\": \"pylint-dev__astroid-1196\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 15}, {\"instance_ids\": \"pylint-dev__astroid-1866\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 6}, {\"instance_ids\": \"pylint-dev__astroid-1268\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 4}, {\"instance_ids\": \"pyvista__pyvista-4315\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 21}, {\"instance_ids\": \"pydicom__pydicom-1694\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 2}, {\"instance_ids\": \"pydicom__pydicom-1413\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 19}, {\"instance_ids\": \"pydicom__pydicom-901\", \"num_files\": 1, \"num_edits\": 3, \"num_imports\": 0, \"num_lines\": 28}, {\"instance_ids\": \"pydicom__pydicom-1139\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 20}, {\"instance_ids\": \"pydicom__pydicom-1256\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 3}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_num_files = 10\n",
    "trunc_num_edits = 50\n",
    "trunc_num_lines = 100\n",
    "trunc_num_imports = 10\n",
    "# Make charts\n",
    "fchart = alt.Chart(df[df.num_files <= trunc_num_files]).mark_bar().encode(\n",
    "    x=alt.X('num_files:O', bin=True, title='Number of Files'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Files per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "echart = alt.Chart(df[df.num_edits <= trunc_num_edits]).mark_bar().encode(\n",
    "    x=alt.X('num_edits:O', bin=True, title='Number of Edits'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Edits per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "lchart = alt.Chart(df[df.num_lines <= trunc_num_lines]).mark_bar().encode(\n",
    "    x=alt.X('num_lines:O', bin=True, title='Number of Lines'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Lines per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "\n",
    "ichart = alt.Chart(df[df.num_imports <= trunc_num_imports]).mark_bar().encode(\n",
    "    x=alt.X('num_imports:O', bin=True, title='Number of Imports'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Imports per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "\n",
    "chart = fchart | echart | lchart | ichart\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Counts\n",
      "Percentage of 0: 0.0\n",
      "Percentage of 1: 100.0\n",
      "Percentage of 2: 0.0\n",
      "Percentage of 3: 0.0\n",
      "Percentage of 4: 0.0\n",
      "Percentage of 5: 0.0\n",
      "Percentage of 6: 0.0\n",
      "Percentage of 7: 0.0\n",
      "Percentage of 8: 0.0\n",
      "Percentage of 9: 0.0\n",
      "Percentage of 10: 0.0\n",
      "Cumulative (<= 10): 100.0\n",
      "Import Counts\n",
      "Percentage of 0: 86.95652173913044\n",
      "Percentage of 1: 4.3478260869565215\n",
      "Percentage of 2: 4.3478260869565215\n",
      "Percentage of 3: 4.3478260869565215\n",
      "Percentage of 4: 0.0\n",
      "Percentage of 5: 0.0\n",
      "Cumulative (<= 5): 99.99999999999999\n",
      "Edit Counts\n",
      "Percentage of 0: 0.0\n",
      "Percentage of 1: 52.17391304347826\n",
      "Percentage of 2: 34.78260869565217\n",
      "Percentage of 3: 13.043478260869565\n",
      "Percentage of 4: 0.0\n",
      "Percentage of 5: 0.0\n",
      "Percentage of 6: 0.0\n",
      "Percentage of 7: 0.0\n",
      "Percentage of 8: 0.0\n",
      "Percentage of 9: 0.0\n",
      "Percentage of 10: 0.0\n",
      "Cumulative (<= 10): 100.0\n",
      "Line Counts\n",
      "Cumulative so far (<= 10): 52.17391304347826\n",
      "Cumulative so far (<= 20): 73.91304347826085\n",
      "Cumulative so far (<= 30): 95.65217391304344\n",
      "Cumulative so far (<= 40): 99.99999999999996\n",
      "Cumulative so far (<= 50): 99.99999999999996\n",
      "Cumulative so far (<= 60): 99.99999999999996\n",
      "Cumulative so far (<= 70): 99.99999999999996\n",
      "Cumulative so far (<= 80): 99.99999999999996\n",
      "Cumulative so far (<= 90): 99.99999999999996\n",
      "Cumulative so far (<= 100): 99.99999999999996\n"
     ]
    }
   ],
   "source": [
    "print(\"File Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 11):\n",
    "    nums = len(df[df['num_files'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    print(f\"Percentage of {count}: {percent}\") \n",
    "    cumulative += percent\n",
    "print(f\"Cumulative (<= {count}): {cumulative}\")\n",
    "print(\"Import Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 6):\n",
    "    nums = len(df[df['num_imports'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    print(f\"Percentage of {count}: {percent}\")\n",
    "    cumulative += percent\n",
    "print(f\"Cumulative (<= {count}): {cumulative}\")\n",
    "print(\"Edit Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 11):\n",
    "    nums = len(df[df['num_edits'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    print(f\"Percentage of {count}: {percent}\")\n",
    "    cumulative += percent\n",
    "print(f\"Cumulative (<= {count}): {cumulative}\")\n",
    "print(\"Line Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 101):\n",
    "    nums = len(df[df['num_lines'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    cumulative += percent\n",
    "    if count > 0 and count % 10 == 0:\n",
    "        print(f\"Cumulative so far (<= {count}): {cumulative}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./working_stage/gold_retrieved_princeton-nlp__SWE-bench_Lite_dev.pkl\n",
      "Instance ID: sqlfluff__sqlfluff-1625.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/rules/L031.py b/src/sqlfluff/rules/L031.py\n",
      "--- a/src/sqlfluff/rules/L031.py\n",
      "+++ b/src/sqlfluff/rules/L031.py\n",
      "@@ -211,7 +211,7 @@ def _lint_aliases_in_join(\n",
      "             violation_buff.append(\n",
      "                 LintResult(\n",
      "                     anchor=alias_info.alias_identifier_ref,\n",
      "-                    description=\"Avoid using aliases in join condition\",\n",
      "+                    description=\"Avoid aliases in from clauses and join conditions.\",\n",
      "                     fixes=fixes,\n",
      "                 )\n",
      "             )\n",
      "\n",
      "Instance ID: sqlfluff__sqlfluff-2419.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/rules/L060.py b/src/sqlfluff/rules/L060.py\n",
      "--- a/src/sqlfluff/rules/L060.py\n",
      "+++ b/src/sqlfluff/rules/L060.py\n",
      "@@ -59,4 +59,8 @@ def _eval(self, context: RuleContext) -> Optional[LintResult]:\n",
      "             ],\n",
      "         )\n",
      " \n",
      "-        return LintResult(context.segment, [fix])\n",
      "+        return LintResult(\n",
      "+            anchor=context.segment,\n",
      "+            fixes=[fix],\n",
      "+            description=f\"Use 'COALESCE' instead of '{context.segment.raw_upper}'.\",\n",
      "+        )\n",
      "\n",
      "Instance ID: sqlfluff__sqlfluff-1733.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/rules/L039.py b/src/sqlfluff/rules/L039.py\n",
      "--- a/src/sqlfluff/rules/L039.py\n",
      "+++ b/src/sqlfluff/rules/L039.py\n",
      "@@ -44,7 +44,9 @@ def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n",
      "                 # This is to avoid indents\n",
      "                 if not prev_newline:\n",
      "                     prev_whitespace = seg\n",
      "-                prev_newline = False\n",
      "+                # We won't set prev_newline to False, just for whitespace\n",
      "+                # in case there's multiple indents, inserted by other rule\n",
      "+                # fixes (see #1713)\n",
      "             elif seg.is_type(\"comment\"):\n",
      "                 prev_newline = False\n",
      "                 prev_whitespace = None\n",
      "\n",
      "Instance ID: sqlfluff__sqlfluff-1517.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/core/parser/helpers.py b/src/sqlfluff/core/parser/helpers.py\n",
      "--- a/src/sqlfluff/core/parser/helpers.py\n",
      "+++ b/src/sqlfluff/core/parser/helpers.py\n",
      "@@ -2,6 +2,7 @@\n",
      " \n",
      " from typing import Tuple, List, Any, Iterator, TYPE_CHECKING\n",
      " \n",
      "+from sqlfluff.core.errors import SQLParseError\n",
      " from sqlfluff.core.string_helpers import curtail_string\n",
      " \n",
      " if TYPE_CHECKING:\n",
      "@@ -26,11 +27,11 @@ def check_still_complete(\n",
      "     \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n",
      "     initial_str = join_segments_raw(segments_in)\n",
      "     current_str = join_segments_raw(matched_segments + unmatched_segments)\n",
      "-    if initial_str != current_str:  # pragma: no cover\n",
      "-        raise RuntimeError(\n",
      "-            \"Dropped elements in sequence matching! {!r} != {!r}\".format(\n",
      "-                initial_str, current_str\n",
      "-            )\n",
      "+\n",
      "+    if initial_str != current_str:\n",
      "+        raise SQLParseError(\n",
      "+            f\"Could not parse: {current_str}\",\n",
      "+            segment=unmatched_segments[0],\n",
      "         )\n",
      "     return True\n",
      " \n",
      "\n",
      "Instance ID: sqlfluff__sqlfluff-1763.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n",
      "--- a/src/sqlfluff/core/linter/linted_file.py\n",
      "+++ b/src/sqlfluff/core/linter/linted_file.py\n",
      "@@ -7,6 +7,8 @@\n",
      " \n",
      " import os\n",
      " import logging\n",
      "+import shutil\n",
      "+import tempfile\n",
      " from typing import (\n",
      "     Any,\n",
      "     Iterable,\n",
      "@@ -493,7 +495,24 @@ def persist_tree(self, suffix: str = \"\") -> bool:\n",
      "             if suffix:\n",
      "                 root, ext = os.path.splitext(fname)\n",
      "                 fname = root + suffix + ext\n",
      "-            # Actually write the file.\n",
      "-            with open(fname, \"w\", encoding=self.encoding) as f:\n",
      "-                f.write(write_buff)\n",
      "+            self._safe_create_replace_file(fname, write_buff, self.encoding)\n",
      "         return success\n",
      "+\n",
      "+    @staticmethod\n",
      "+    def _safe_create_replace_file(fname, write_buff, encoding):\n",
      "+        # Write to a temporary file first, so in case of encoding or other\n",
      "+        # issues, we don't delete or corrupt the user's existing file.\n",
      "+        dirname, basename = os.path.split(fname)\n",
      "+        with tempfile.NamedTemporaryFile(\n",
      "+            mode=\"w\",\n",
      "+            encoding=encoding,\n",
      "+            prefix=basename,\n",
      "+            dir=dirname,\n",
      "+            suffix=os.path.splitext(fname)[1],\n",
      "+            delete=False,\n",
      "+        ) as tmp:\n",
      "+            tmp.file.write(write_buff)\n",
      "+            tmp.flush()\n",
      "+            os.fsync(tmp.fileno())\n",
      "+        # Once the temp file is safely written, replace the existing file.\n",
      "+        shutil.move(tmp.name, fname)\n",
      "\n",
      "Instance ID: marshmallow-code__marshmallow-1359.\n",
      "Patch:\n",
      "diff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\n",
      "--- a/src/marshmallow/fields.py\n",
      "+++ b/src/marshmallow/fields.py\n",
      "@@ -1114,7 +1114,7 @@ def _bind_to_schema(self, field_name, schema):\n",
      "         super()._bind_to_schema(field_name, schema)\n",
      "         self.format = (\n",
      "             self.format\n",
      "-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n",
      "+            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\n",
      "             or self.DEFAULT_FORMAT\n",
      "         )\n",
      " \n",
      "\n",
      "Instance ID: marshmallow-code__marshmallow-1343.\n",
      "Patch:\n",
      "diff --git a/src/marshmallow/schema.py b/src/marshmallow/schema.py\n",
      "--- a/src/marshmallow/schema.py\n",
      "+++ b/src/marshmallow/schema.py\n",
      "@@ -877,7 +877,7 @@ def _invoke_field_validators(self, unmarshal, data, many):\n",
      "                 for idx, item in enumerate(data):\n",
      "                     try:\n",
      "                         value = item[field_obj.attribute or field_name]\n",
      "-                    except KeyError:\n",
      "+                    except (KeyError, TypeError):\n",
      "                         pass\n",
      "                     else:\n",
      "                         validated_value = unmarshal.call_and_store(\n",
      "@@ -892,7 +892,7 @@ def _invoke_field_validators(self, unmarshal, data, many):\n",
      "             else:\n",
      "                 try:\n",
      "                     value = data[field_obj.attribute or field_name]\n",
      "-                except KeyError:\n",
      "+                except (KeyError, TypeError):\n",
      "                     pass\n",
      "                 else:\n",
      "                     validated_value = unmarshal.call_and_store(\n",
      "\n",
      "Instance ID: pvlib__pvlib-python-1707.\n",
      "Patch:\n",
      "diff --git a/pvlib/iam.py b/pvlib/iam.py\n",
      "--- a/pvlib/iam.py\n",
      "+++ b/pvlib/iam.py\n",
      "@@ -175,8 +175,12 @@ def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):\n",
      "     n2costheta2 = n2 * costheta\n",
      " \n",
      "     # reflectance of s-, p-polarized, and normal light by the first interface\n",
      "-    rho12_s = ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n",
      "-    rho12_p = ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n",
      "+    with np.errstate(divide='ignore', invalid='ignore'):\n",
      "+        rho12_s = \\\n",
      "+            ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n",
      "+        rho12_p = \\\n",
      "+            ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n",
      "+\n",
      "     rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2\n",
      " \n",
      "     # transmittance through the first interface\n",
      "@@ -208,13 +212,22 @@ def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):\n",
      "         tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)\n",
      " \n",
      "     # transmittance after absorption in the glass\n",
      "-    tau_s *= np.exp(-K * L / costheta)\n",
      "-    tau_p *= np.exp(-K * L / costheta)\n",
      "+    with np.errstate(divide='ignore', invalid='ignore'):\n",
      "+        tau_s *= np.exp(-K * L / costheta)\n",
      "+        tau_p *= np.exp(-K * L / costheta)\n",
      "+\n",
      "     tau_0 *= np.exp(-K * L)\n",
      " \n",
      "     # incidence angle modifier\n",
      "     iam = (tau_s + tau_p) / 2 / tau_0\n",
      " \n",
      "+    # for light coming from behind the plane, none can enter the module\n",
      "+    # when n2 > 1, this is already the case\n",
      "+    if np.isclose(n2, 1).any():\n",
      "+        iam = np.where(aoi >= 90, 0, iam)\n",
      "+        if isinstance(aoi, pd.Series):\n",
      "+            iam = pd.Series(iam, index=aoi.index)\n",
      "+\n",
      "     return iam\n",
      " \n",
      " \n",
      "\n",
      "Instance ID: pvlib__pvlib-python-1072.\n",
      "Patch:\n",
      "diff --git a/pvlib/temperature.py b/pvlib/temperature.py\n",
      "--- a/pvlib/temperature.py\n",
      "+++ b/pvlib/temperature.py\n",
      "@@ -599,8 +599,9 @@ def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,\n",
      "     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n",
      "     # the value doesn't matter -- rather than recreate it here, just assume\n",
      "     # it's the same as the second timedelta:\n",
      "-    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\n",
      "-    timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)\n",
      "+    timedelta_seconds = poa_global.index.to_series().diff().dt.total_seconds()\n",
      "+    timedelta_hours = timedelta_seconds / 3600\n",
      "+    timedelta_hours.iloc[0] = timedelta_hours.iloc[1]\n",
      " \n",
      "     tamb_array = temp_air + 273.15\n",
      "     sun_array = poa_global * absorp\n",
      "\n",
      "Instance ID: pvlib__pvlib-python-1854.\n",
      "Patch:\n",
      "diff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n",
      "--- a/pvlib/pvsystem.py\n",
      "+++ b/pvlib/pvsystem.py\n",
      "@@ -101,10 +101,11 @@ class PVSystem:\n",
      " \n",
      "     Parameters\n",
      "     ----------\n",
      "-    arrays : iterable of Array, optional\n",
      "-        List of arrays that are part of the system. If not specified\n",
      "-        a single array is created from the other parameters (e.g.\n",
      "-        `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n",
      "+    arrays : Array or iterable of Array, optional\n",
      "+        An Array or list of arrays that are part of the system. If not\n",
      "+        specified a single array is created from the other parameters (e.g.\n",
      "+        `surface_tilt`, `surface_azimuth`). If specified as a list, the list\n",
      "+        must contain at least one Array;\n",
      "         if length of arrays is 0 a ValueError is raised. If `arrays` is\n",
      "         specified the following PVSystem parameters are ignored:\n",
      " \n",
      "@@ -220,6 +221,8 @@ def __init__(self,\n",
      "                 strings_per_inverter,\n",
      "                 array_losses_parameters,\n",
      "             ),)\n",
      "+        elif isinstance(arrays, Array):\n",
      "+            self.arrays = (arrays,)\n",
      "         elif len(arrays) == 0:\n",
      "             raise ValueError(\"PVSystem must have at least one Array. \"\n",
      "                              \"If you want to create a PVSystem instance \"\n",
      "\n",
      "Instance ID: pvlib__pvlib-python-1154.\n",
      "Patch:\n",
      "diff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n",
      "--- a/pvlib/irradiance.py\n",
      "+++ b/pvlib/irradiance.py\n",
      "@@ -886,8 +886,9 @@ def reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,\n",
      "     # these are the () and [] sub-terms of the second term of eqn 8\n",
      "     term1 = 1 - AI\n",
      "     term2 = 0.5 * (1 + tools.cosd(surface_tilt))\n",
      "-    term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)\n",
      "-\n",
      "+    with np.errstate(invalid='ignore', divide='ignore'):\n",
      "+        hb_to_ghi = np.where(ghi == 0, 0, np.divide(HB, ghi))\n",
      "+    term3 = 1 + np.sqrt(hb_to_ghi) * (tools.sind(0.5 * surface_tilt)**3)\n",
      "     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)\n",
      "     sky_diffuse = np.maximum(sky_diffuse, 0)\n",
      " \n",
      "\n",
      "Instance ID: pylint-dev__astroid-1333.\n",
      "Patch:\n",
      "diff --git a/astroid/modutils.py b/astroid/modutils.py\n",
      "--- a/astroid/modutils.py\n",
      "+++ b/astroid/modutils.py\n",
      "@@ -297,6 +297,9 @@ def _get_relative_base_path(filename, path_to_check):\n",
      "     if os.path.normcase(real_filename).startswith(path_to_check):\n",
      "         importable_path = real_filename\n",
      " \n",
      "+    # if \"var\" in path_to_check:\n",
      "+    #     breakpoint()\n",
      "+\n",
      "     if importable_path:\n",
      "         base_path = os.path.splitext(importable_path)[0]\n",
      "         relative_base_path = base_path[len(path_to_check) :]\n",
      "@@ -307,8 +310,11 @@ def _get_relative_base_path(filename, path_to_check):\n",
      " \n",
      " def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):\n",
      "     filename = os.path.expanduser(_path_from_filename(filename))\n",
      "+    paths_to_check = sys.path.copy()\n",
      "+    if path:\n",
      "+        paths_to_check += path\n",
      "     for pathname in itertools.chain(\n",
      "-        path or [], map(_cache_normalize_path, sys.path), sys.path\n",
      "+        paths_to_check, map(_cache_normalize_path, paths_to_check)\n",
      "     ):\n",
      "         if not pathname:\n",
      "             continue\n",
      "\n",
      "Instance ID: pylint-dev__astroid-1196.\n",
      "Patch:\n",
      "diff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n",
      "--- a/astroid/nodes/node_classes.py\n",
      "+++ b/astroid/nodes/node_classes.py\n",
      "@@ -2346,24 +2346,33 @@ def itered(self):\n",
      "         \"\"\"\n",
      "         return [key for (key, _) in self.items]\n",
      " \n",
      "-    def getitem(self, index, context=None):\n",
      "+    def getitem(\n",
      "+        self, index: Const | Slice, context: InferenceContext | None = None\n",
      "+    ) -> NodeNG:\n",
      "         \"\"\"Get an item from this node.\n",
      " \n",
      "         :param index: The node to use as a subscript index.\n",
      "-        :type index: Const or Slice\n",
      " \n",
      "         :raises AstroidTypeError: When the given index cannot be used as a\n",
      "             subscript index, or if this node is not subscriptable.\n",
      "         :raises AstroidIndexError: If the given index does not exist in the\n",
      "             dictionary.\n",
      "         \"\"\"\n",
      "+        # pylint: disable-next=import-outside-toplevel; circular import\n",
      "+        from astroid.helpers import safe_infer\n",
      "+\n",
      "         for key, value in self.items:\n",
      "             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n",
      "             if isinstance(key, DictUnpack):\n",
      "+                inferred_value = safe_infer(value, context)\n",
      "+                if not isinstance(inferred_value, Dict):\n",
      "+                    continue\n",
      "+\n",
      "                 try:\n",
      "-                    return value.getitem(index, context)\n",
      "+                    return inferred_value.getitem(index, context)\n",
      "                 except (AstroidTypeError, AstroidIndexError):\n",
      "                     continue\n",
      "+\n",
      "             for inferredkey in key.infer(context):\n",
      "                 if inferredkey is util.Uninferable:\n",
      "                     continue\n",
      "\n",
      "Instance ID: pylint-dev__astroid-1866.\n",
      "Patch:\n",
      "diff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py\n",
      "--- a/astroid/brain/brain_builtin_inference.py\n",
      "+++ b/astroid/brain/brain_builtin_inference.py\n",
      "@@ -954,8 +954,10 @@ def _infer_str_format_call(\n",
      " \n",
      "     try:\n",
      "         formatted_string = format_template.format(*pos_values, **keyword_values)\n",
      "-    except (IndexError, KeyError):\n",
      "-        # If there is an IndexError there are too few arguments to interpolate\n",
      "+    except (IndexError, KeyError, TypeError, ValueError):\n",
      "+        # IndexError: there are too few arguments to interpolate\n",
      "+        # TypeError: Unsupported format string\n",
      "+        # ValueError: Unknown format code\n",
      "         return iter([util.Uninferable])\n",
      " \n",
      "     return iter([nodes.const_factory(formatted_string)])\n",
      "\n",
      "Instance ID: pylint-dev__astroid-1268.\n",
      "Patch:\n",
      "diff --git a/astroid/nodes/as_string.py b/astroid/nodes/as_string.py\n",
      "--- a/astroid/nodes/as_string.py\n",
      "+++ b/astroid/nodes/as_string.py\n",
      "@@ -36,6 +36,7 @@\n",
      "         MatchSingleton,\n",
      "         MatchStar,\n",
      "         MatchValue,\n",
      "+        Unknown,\n",
      "     )\n",
      " \n",
      " # pylint: disable=unused-argument\n",
      "@@ -643,6 +644,9 @@ def visit_property(self, node):\n",
      "     def visit_evaluatedobject(self, node):\n",
      "         return node.original.accept(self)\n",
      " \n",
      "+    def visit_unknown(self, node: \"Unknown\") -> str:\n",
      "+        return str(node)\n",
      "+\n",
      " \n",
      " def _import_string(names):\n",
      "     \"\"\"return a list of (name, asname) formatted as a string\"\"\"\n",
      "\n",
      "Instance ID: pyvista__pyvista-4315.\n",
      "Patch:\n",
      "diff --git a/pyvista/core/grid.py b/pyvista/core/grid.py\n",
      "--- a/pyvista/core/grid.py\n",
      "+++ b/pyvista/core/grid.py\n",
      "@@ -135,23 +135,30 @@ def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n",
      "                     self.shallow_copy(args[0])\n",
      "             elif isinstance(args[0], (str, pathlib.Path)):\n",
      "                 self._from_file(args[0], **kwargs)\n",
      "-            elif isinstance(args[0], np.ndarray):\n",
      "-                self._from_arrays(args[0], None, None, check_duplicates)\n",
      "+            elif isinstance(args[0], (np.ndarray, Sequence)):\n",
      "+                self._from_arrays(np.asanyarray(args[0]), None, None, check_duplicates)\n",
      "             else:\n",
      "                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n",
      " \n",
      "         elif len(args) == 3 or len(args) == 2:\n",
      "-            arg0_is_arr = isinstance(args[0], np.ndarray)\n",
      "-            arg1_is_arr = isinstance(args[1], np.ndarray)\n",
      "+            arg0_is_arr = isinstance(args[0], (np.ndarray, Sequence))\n",
      "+            arg1_is_arr = isinstance(args[1], (np.ndarray, Sequence))\n",
      "             if len(args) == 3:\n",
      "-                arg2_is_arr = isinstance(args[2], np.ndarray)\n",
      "+                arg2_is_arr = isinstance(args[2], (np.ndarray, Sequence))\n",
      "             else:\n",
      "                 arg2_is_arr = False\n",
      " \n",
      "             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n",
      "-                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n",
      "+                self._from_arrays(\n",
      "+                    np.asanyarray(args[0]),\n",
      "+                    np.asanyarray(args[1]),\n",
      "+                    np.asanyarray(args[2]),\n",
      "+                    check_duplicates,\n",
      "+                )\n",
      "             elif all([arg0_is_arr, arg1_is_arr]):\n",
      "-                self._from_arrays(args[0], args[1], None, check_duplicates)\n",
      "+                self._from_arrays(\n",
      "+                    np.asanyarray(args[0]), np.asanyarray(args[1]), None, check_duplicates\n",
      "+                )\n",
      "             else:\n",
      "                 raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n",
      " \n",
      "\n",
      "Instance ID: pydicom__pydicom-1694.\n",
      "Patch:\n",
      "diff --git a/pydicom/dataset.py b/pydicom/dataset.py\n",
      "--- a/pydicom/dataset.py\n",
      "+++ b/pydicom/dataset.py\n",
      "@@ -2492,8 +2492,8 @@ def to_json_dict(\n",
      "         json_dataset = {}\n",
      "         for key in self.keys():\n",
      "             json_key = '{:08X}'.format(key)\n",
      "-            data_element = self[key]\n",
      "             try:\n",
      "+                data_element = self[key]\n",
      "                 json_dataset[json_key] = data_element.to_json_dict(\n",
      "                     bulk_data_element_handler=bulk_data_element_handler,\n",
      "                     bulk_data_threshold=bulk_data_threshold\n",
      "\n",
      "Instance ID: pydicom__pydicom-1413.\n",
      "Patch:\n",
      "diff --git a/pydicom/dataelem.py b/pydicom/dataelem.py\n",
      "--- a/pydicom/dataelem.py\n",
      "+++ b/pydicom/dataelem.py\n",
      "@@ -433,13 +433,24 @@ def value(self) -> Any:\n",
      "     @value.setter\n",
      "     def value(self, val: Any) -> None:\n",
      "         \"\"\"Convert (if necessary) and set the value of the element.\"\"\"\n",
      "+        # Ignore backslash characters in these VRs, based on:\n",
      "+        # * Which str VRs can have backslashes in Part 5, Section 6.2\n",
      "+        # * All byte VRs\n",
      "+        exclusions = [\n",
      "+            'LT', 'OB', 'OD', 'OF', 'OL', 'OV', 'OW', 'ST', 'UN', 'UT',\n",
      "+            'OB/OW', 'OW/OB', 'OB or OW', 'OW or OB',\n",
      "+            # Probably not needed\n",
      "+            'AT', 'FD', 'FL', 'SQ', 'SS', 'SL', 'UL',\n",
      "+        ]\n",
      "+\n",
      "         # Check if is a string with multiple values separated by '\\'\n",
      "         # If so, turn them into a list of separate strings\n",
      "         #  Last condition covers 'US or SS' etc\n",
      "-        if isinstance(val, (str, bytes)) and self.VR not in \\\n",
      "-                ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL',\n",
      "-                 'SQ', 'SS', 'UL', 'OB/OW', 'OW/OB', 'OB or OW',\n",
      "-                 'OW or OB', 'UN'] and 'US' not in self.VR:\n",
      "+        if (\n",
      "+            isinstance(val, (str, bytes))\n",
      "+            and self.VR not in exclusions\n",
      "+            and 'US' not in self.VR\n",
      "+        ):\n",
      "             try:\n",
      "                 if _backslash_str in val:\n",
      "                     val = cast(str, val).split(_backslash_str)\n",
      "\n",
      "Instance ID: pydicom__pydicom-1139.\n",
      "Patch:\n",
      "diff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n",
      "--- a/pydicom/valuerep.py\n",
      "+++ b/pydicom/valuerep.py\n",
      "@@ -1,6 +1,5 @@\n",
      " # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n",
      " \"\"\"Special classes for DICOM value representations (VR)\"\"\"\n",
      "-from copy import deepcopy\n",
      " from decimal import Decimal\n",
      " import re\n",
      " \n",
      "@@ -750,6 +749,25 @@ def __ne__(self, other):\n",
      "     def __str__(self):\n",
      "         return '='.join(self.components).__str__()\n",
      " \n",
      "+    def __next__(self):\n",
      "+        # Get next character or stop iteration\n",
      "+        if self._i < self._rep_len:\n",
      "+            c = self._str_rep[self._i]\n",
      "+            self._i += 1\n",
      "+            return c\n",
      "+        else:\n",
      "+            raise StopIteration\n",
      "+\n",
      "+    def __iter__(self):\n",
      "+        # Get string rep. and length, initialize index counter\n",
      "+        self._str_rep = self.__str__()\n",
      "+        self._rep_len = len(self._str_rep)\n",
      "+        self._i = 0\n",
      "+        return self\n",
      "+\n",
      "+    def __contains__(self, x):\n",
      "+        return x in self.__str__()\n",
      "+\n",
      "     def __repr__(self):\n",
      "         return '='.join(self.components).__repr__()\n",
      " \n",
      "\n",
      "Instance ID: pydicom__pydicom-1256.\n",
      "Patch:\n",
      "diff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n",
      "--- a/pydicom/jsonrep.py\n",
      "+++ b/pydicom/jsonrep.py\n",
      "@@ -226,7 +226,8 @@ def get_sequence_item(self, value):\n",
      "                     value_key = unique_value_keys[0]\n",
      "                     elem = DataElement.from_json(\n",
      "                         self.dataset_class, key, vr,\n",
      "-                        val[value_key], value_key\n",
      "+                        val[value_key], value_key,\n",
      "+                        self.bulk_data_element_handler\n",
      "                     )\n",
      "                 ds.add(elem)\n",
      "         return ds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sqlfluff__sqlfluff-1625',\n",
       " 'sqlfluff__sqlfluff-2419',\n",
       " 'sqlfluff__sqlfluff-1733',\n",
       " 'sqlfluff__sqlfluff-1517',\n",
       " 'sqlfluff__sqlfluff-1763',\n",
       " 'marshmallow-code__marshmallow-1359',\n",
       " 'marshmallow-code__marshmallow-1343',\n",
       " 'pvlib__pvlib-python-1707',\n",
       " 'pvlib__pvlib-python-1072',\n",
       " 'pvlib__pvlib-python-1854',\n",
       " 'pvlib__pvlib-python-1154',\n",
       " 'pylint-dev__astroid-1333',\n",
       " 'pylint-dev__astroid-1196',\n",
       " 'pylint-dev__astroid-1866',\n",
       " 'pylint-dev__astroid-1268',\n",
       " 'pyvista__pyvista-4315',\n",
       " 'pydicom__pydicom-1694',\n",
       " 'pydicom__pydicom-1413',\n",
       " 'pydicom__pydicom-1139',\n",
       " 'pydicom__pydicom-1256']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any existing patch file of the form essai-<instance_id>.patch\n",
    "for f in os.listdir(working_stage):\n",
    "    if f.startswith(f\"essai\") and f.endswith(\".patch\"):\n",
    "        os.remove(f\"{working_stage}/{f}\")\n",
    "retriever = GoldenRetriever(dataset_name=dataset, split=split, num_shards=None, shard_id=None)\n",
    "res = []\n",
    "for instance_id in simple_df['instance_ids']:\n",
    "    patch = retriever.gold_patches[instance_id]\n",
    "    print(f\"Instance ID: {instance_id}.\")\n",
    "    print(f\"Patch:\\n{patch}\")\n",
    "    patch_file = f\"{working_stage}/essai-{instance_id}.patch\"\n",
    "    with open(patch_file, 'w') as f:\n",
    "        f.write(f\"Instance ID: {instance_id}\\n\")\n",
    "        f.write(f\"{patch}\")\n",
    "    res.append(instance_id)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
