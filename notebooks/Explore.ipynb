{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import os\n",
    "from common.interfaces import RetrievedFile, FileDisplayLevel\n",
    "from retrieval.gold import GoldenRetriever\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "\n",
    "config = tomllib.load(open(\"configs/main.toml\", \"rb\"))\n",
    "working_stage = config[\"working_stage\"]\n",
    "dataset = \"princeton-nlp/SWE-bench_Lite\"\n",
    "split = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./working_stage/gold_retrieved_princeton-nlp__SWE-bench_Lite_dev.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12/23'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_stats_df(dataset, split, filter_by_count=None, code_only=True):\n",
    "    retriever = GoldenRetriever(dataset_name=dataset, split=split, num_shards=None, shard_id=None)\n",
    "    stats = retriever.collect_stats(code_only=True)\n",
    "    instance_ids = [k for k in stats['num_files'].keys()]\n",
    "    num_files = [v for v in stats['num_files'].values()]\n",
    "    num_edits = [v for v in stats['num_edits'].values()]\n",
    "    num_lines = [v for v in stats['num_lines'].values()]\n",
    "    num_imports = [v for v in stats['num_imports'].values()]\n",
    "\n",
    "    data = {\n",
    "        'instance_ids': instance_ids,\n",
    "        'num_files': num_files,\n",
    "        'num_edits': num_edits,\n",
    "        'num_imports': num_imports,\n",
    "        'num_lines': num_lines\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    if filter_by_count is not None:\n",
    "        df = df[df['num_files'] == filter_by_count]\n",
    "    return df\n",
    "\n",
    "df = make_stats_df(dataset, split, filter_by_count=None, code_only=True)\n",
    "simple_df = df[(df['num_files'] == 1) & (df['num_edits'] == 1)]\n",
    "f\"{len(simple_df)}/{len(df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d.vega-embed details,\n",
       "  #altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-aa47b64ef5e94ccab795f50c2ee33a2d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_files\", \"title\": \"Number of Files\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Files per Instance\", \"width\": 400}, {\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_edits\", \"title\": \"Number of Edits\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Edits per Instance\", \"width\": 400}, {\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_lines\", \"title\": \"Number of Lines\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Lines per Instance\", \"width\": 400}, {\"data\": {\"name\": \"data-5c1dadf12b6504d095f5a414c8b1d6c6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"bin\": true, \"field\": \"num_imports\", \"title\": \"Number of Imports\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"count\", \"title\": \"Number of Instances\", \"type\": \"quantitative\"}}, \"height\": 250, \"title\": \"Number of Imports per Instance\", \"width\": 400}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-5c1dadf12b6504d095f5a414c8b1d6c6\": [{\"instance_ids\": \"sqlfluff__sqlfluff-1625\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 2}, {\"instance_ids\": \"sqlfluff__sqlfluff-2419\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 6}, {\"instance_ids\": \"sqlfluff__sqlfluff-1733\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 4}, {\"instance_ids\": \"sqlfluff__sqlfluff-1517\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 1, \"num_lines\": 11}, {\"instance_ids\": \"sqlfluff__sqlfluff-1763\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 2, \"num_lines\": 25}, {\"instance_ids\": \"marshmallow-code__marshmallow-1359\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 2}, {\"instance_ids\": \"marshmallow-code__marshmallow-1343\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 4}, {\"instance_ids\": \"pvlib__pvlib-python-1707\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 21}, {\"instance_ids\": \"pvlib__pvlib-python-1072\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 5}, {\"instance_ids\": \"pvlib__pvlib-python-1606\", \"num_files\": 1, \"num_edits\": 3, \"num_imports\": 0, \"num_lines\": 36}, {\"instance_ids\": \"pvlib__pvlib-python-1854\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 11}, {\"instance_ids\": \"pvlib__pvlib-python-1154\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 5}, {\"instance_ids\": \"pylint-dev__astroid-1978\", \"num_files\": 1, \"num_edits\": 3, \"num_imports\": 3, \"num_lines\": 27}, {\"instance_ids\": \"pylint-dev__astroid-1333\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 8}, {\"instance_ids\": \"pylint-dev__astroid-1196\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 15}, {\"instance_ids\": \"pylint-dev__astroid-1866\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 6}, {\"instance_ids\": \"pylint-dev__astroid-1268\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 4}, {\"instance_ids\": \"pyvista__pyvista-4315\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 21}, {\"instance_ids\": \"pydicom__pydicom-1694\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 2}, {\"instance_ids\": \"pydicom__pydicom-1413\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 19}, {\"instance_ids\": \"pydicom__pydicom-901\", \"num_files\": 1, \"num_edits\": 3, \"num_imports\": 0, \"num_lines\": 28}, {\"instance_ids\": \"pydicom__pydicom-1139\", \"num_files\": 1, \"num_edits\": 2, \"num_imports\": 0, \"num_lines\": 20}, {\"instance_ids\": \"pydicom__pydicom-1256\", \"num_files\": 1, \"num_edits\": 1, \"num_imports\": 0, \"num_lines\": 3}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_num_files = 10\n",
    "trunc_num_edits = 50\n",
    "trunc_num_lines = 100\n",
    "trunc_num_imports = 10\n",
    "# Make charts\n",
    "fchart = alt.Chart(df[df.num_files <= trunc_num_files]).mark_bar().encode(\n",
    "    x=alt.X('num_files:O', bin=True, title='Number of Files'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Files per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "echart = alt.Chart(df[df.num_edits <= trunc_num_edits]).mark_bar().encode(\n",
    "    x=alt.X('num_edits:O', bin=True, title='Number of Edits'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Edits per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "lchart = alt.Chart(df[df.num_lines <= trunc_num_lines]).mark_bar().encode(\n",
    "    x=alt.X('num_lines:O', bin=True, title='Number of Lines'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Lines per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "\n",
    "ichart = alt.Chart(df[df.num_imports <= trunc_num_imports]).mark_bar().encode(\n",
    "    x=alt.X('num_imports:O', bin=True, title='Number of Imports'),\n",
    "    y=alt.Y('count()', title='Number of Instances'),\n",
    ").properties(\n",
    "    title='Number of Imports per Instance',\n",
    "    width=400,\n",
    "    height=250,\n",
    ")\n",
    "\n",
    "\n",
    "chart = fchart | echart | lchart | ichart\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Counts\n",
      "Percentage of 0: 0.0\n",
      "Percentage of 1: 100.0\n",
      "Percentage of 2: 0.0\n",
      "Percentage of 3: 0.0\n",
      "Percentage of 4: 0.0\n",
      "Percentage of 5: 0.0\n",
      "Percentage of 6: 0.0\n",
      "Percentage of 7: 0.0\n",
      "Percentage of 8: 0.0\n",
      "Percentage of 9: 0.0\n",
      "Percentage of 10: 0.0\n",
      "Cumulative (<= 10): 100.0\n",
      "Import Counts\n",
      "Percentage of 0: 86.95652173913044\n",
      "Percentage of 1: 4.3478260869565215\n",
      "Percentage of 2: 4.3478260869565215\n",
      "Percentage of 3: 4.3478260869565215\n",
      "Percentage of 4: 0.0\n",
      "Percentage of 5: 0.0\n",
      "Cumulative (<= 5): 99.99999999999999\n",
      "Edit Counts\n",
      "Percentage of 0: 0.0\n",
      "Percentage of 1: 52.17391304347826\n",
      "Percentage of 2: 34.78260869565217\n",
      "Percentage of 3: 13.043478260869565\n",
      "Percentage of 4: 0.0\n",
      "Percentage of 5: 0.0\n",
      "Percentage of 6: 0.0\n",
      "Percentage of 7: 0.0\n",
      "Percentage of 8: 0.0\n",
      "Percentage of 9: 0.0\n",
      "Percentage of 10: 0.0\n",
      "Cumulative (<= 10): 100.0\n",
      "Line Counts\n",
      "Cumulative so far (<= 10): 52.17391304347826\n",
      "Cumulative so far (<= 20): 73.91304347826085\n",
      "Cumulative so far (<= 30): 95.65217391304344\n",
      "Cumulative so far (<= 40): 99.99999999999996\n",
      "Cumulative so far (<= 50): 99.99999999999996\n",
      "Cumulative so far (<= 60): 99.99999999999996\n",
      "Cumulative so far (<= 70): 99.99999999999996\n",
      "Cumulative so far (<= 80): 99.99999999999996\n",
      "Cumulative so far (<= 90): 99.99999999999996\n",
      "Cumulative so far (<= 100): 99.99999999999996\n"
     ]
    }
   ],
   "source": [
    "print(\"File Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 11):\n",
    "    nums = len(df[df['num_files'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    print(f\"Percentage of {count}: {percent}\") \n",
    "    cumulative += percent\n",
    "print(f\"Cumulative (<= {count}): {cumulative}\")\n",
    "print(\"Import Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 6):\n",
    "    nums = len(df[df['num_imports'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    print(f\"Percentage of {count}: {percent}\")\n",
    "    cumulative += percent\n",
    "print(f\"Cumulative (<= {count}): {cumulative}\")\n",
    "print(\"Edit Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 11):\n",
    "    nums = len(df[df['num_edits'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    print(f\"Percentage of {count}: {percent}\")\n",
    "    cumulative += percent\n",
    "print(f\"Cumulative (<= {count}): {cumulative}\")\n",
    "print(\"Line Counts\")\n",
    "cumulative = 0\n",
    "for count in range(0, 101):\n",
    "    nums = len(df[df['num_lines'] == count])\n",
    "    percent = (100.0 * nums) / len(df)\n",
    "    cumulative += percent\n",
    "    if count > 0 and count % 10 == 0:\n",
    "        print(f\"Cumulative so far (<= {count}): {cumulative}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cache from ./working_stage/gold_retrieved_princeton-nlp__SWE-bench_Lite_dev.pkl\n",
      "Found 12.\n",
      "                         instance_ids  num_files  num_edits  num_imports  \\\n",
      "0             sqlfluff__sqlfluff-1625          1          1            0   \n",
      "1             sqlfluff__sqlfluff-2419          1          1            0   \n",
      "2             sqlfluff__sqlfluff-1733          1          1            0   \n",
      "5  marshmallow-code__marshmallow-1359          1          1            0   \n",
      "8            pvlib__pvlib-python-1072          1          1            0   \n",
      "\n",
      "   num_lines  \n",
      "0          2  \n",
      "1          6  \n",
      "2          4  \n",
      "5          2  \n",
      "8          5  \n",
      "Instance ID: sqlfluff__sqlfluff-1625.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/rules/L031.py b/src/sqlfluff/rules/L031.py\n",
      "--- a/src/sqlfluff/rules/L031.py\n",
      "+++ b/src/sqlfluff/rules/L031.py\n",
      "@@ -211,7 +211,7 @@ def _lint_aliases_in_join(\n",
      "             violation_buff.append(\n",
      "                 LintResult(\n",
      "                     anchor=alias_info.alias_identifier_ref,\n",
      "-                    description=\"Avoid using aliases in join condition\",\n",
      "+                    description=\"Avoid aliases in from clauses and join conditions.\",\n",
      "                     fixes=fixes,\n",
      "                 )\n",
      "             )\n",
      "\n",
      "Instance ID: sqlfluff__sqlfluff-2419.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/rules/L060.py b/src/sqlfluff/rules/L060.py\n",
      "--- a/src/sqlfluff/rules/L060.py\n",
      "+++ b/src/sqlfluff/rules/L060.py\n",
      "@@ -59,4 +59,8 @@ def _eval(self, context: RuleContext) -> Optional[LintResult]:\n",
      "             ],\n",
      "         )\n",
      " \n",
      "-        return LintResult(context.segment, [fix])\n",
      "+        return LintResult(\n",
      "+            anchor=context.segment,\n",
      "+            fixes=[fix],\n",
      "+            description=f\"Use 'COALESCE' instead of '{context.segment.raw_upper}'.\",\n",
      "+        )\n",
      "\n",
      "Instance ID: sqlfluff__sqlfluff-1733.\n",
      "Patch:\n",
      "diff --git a/src/sqlfluff/rules/L039.py b/src/sqlfluff/rules/L039.py\n",
      "--- a/src/sqlfluff/rules/L039.py\n",
      "+++ b/src/sqlfluff/rules/L039.py\n",
      "@@ -44,7 +44,9 @@ def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n",
      "                 # This is to avoid indents\n",
      "                 if not prev_newline:\n",
      "                     prev_whitespace = seg\n",
      "-                prev_newline = False\n",
      "+                # We won't set prev_newline to False, just for whitespace\n",
      "+                # in case there's multiple indents, inserted by other rule\n",
      "+                # fixes (see #1713)\n",
      "             elif seg.is_type(\"comment\"):\n",
      "                 prev_newline = False\n",
      "                 prev_whitespace = None\n",
      "\n",
      "Instance ID: marshmallow-code__marshmallow-1359.\n",
      "Patch:\n",
      "diff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\n",
      "--- a/src/marshmallow/fields.py\n",
      "+++ b/src/marshmallow/fields.py\n",
      "@@ -1114,7 +1114,7 @@ def _bind_to_schema(self, field_name, schema):\n",
      "         super()._bind_to_schema(field_name, schema)\n",
      "         self.format = (\n",
      "             self.format\n",
      "-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n",
      "+            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\n",
      "             or self.DEFAULT_FORMAT\n",
      "         )\n",
      " \n",
      "\n",
      "Instance ID: pvlib__pvlib-python-1072.\n",
      "Patch:\n",
      "diff --git a/pvlib/temperature.py b/pvlib/temperature.py\n",
      "--- a/pvlib/temperature.py\n",
      "+++ b/pvlib/temperature.py\n",
      "@@ -599,8 +599,9 @@ def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,\n",
      "     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n",
      "     # the value doesn't matter -- rather than recreate it here, just assume\n",
      "     # it's the same as the second timedelta:\n",
      "-    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\n",
      "-    timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)\n",
      "+    timedelta_seconds = poa_global.index.to_series().diff().dt.total_seconds()\n",
      "+    timedelta_hours = timedelta_seconds / 3600\n",
      "+    timedelta_hours.iloc[0] = timedelta_hours.iloc[1]\n",
      " \n",
      "     tamb_array = temp_air + 273.15\n",
      "     sun_array = poa_global * absorp\n",
      "\n",
      "Instance ID: pvlib__pvlib-python-1154.\n",
      "Patch:\n",
      "diff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n",
      "--- a/pvlib/irradiance.py\n",
      "+++ b/pvlib/irradiance.py\n",
      "@@ -886,8 +886,9 @@ def reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,\n",
      "     # these are the () and [] sub-terms of the second term of eqn 8\n",
      "     term1 = 1 - AI\n",
      "     term2 = 0.5 * (1 + tools.cosd(surface_tilt))\n",
      "-    term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)\n",
      "-\n",
      "+    with np.errstate(invalid='ignore', divide='ignore'):\n",
      "+        hb_to_ghi = np.where(ghi == 0, 0, np.divide(HB, ghi))\n",
      "+    term3 = 1 + np.sqrt(hb_to_ghi) * (tools.sind(0.5 * surface_tilt)**3)\n",
      "     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)\n",
      "     sky_diffuse = np.maximum(sky_diffuse, 0)\n",
      " \n",
      "\n",
      "Instance ID: pylint-dev__astroid-1196.\n",
      "Patch:\n",
      "diff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n",
      "--- a/astroid/nodes/node_classes.py\n",
      "+++ b/astroid/nodes/node_classes.py\n",
      "@@ -2346,24 +2346,33 @@ def itered(self):\n",
      "         \"\"\"\n",
      "         return [key for (key, _) in self.items]\n",
      " \n",
      "-    def getitem(self, index, context=None):\n",
      "+    def getitem(\n",
      "+        self, index: Const | Slice, context: InferenceContext | None = None\n",
      "+    ) -> NodeNG:\n",
      "         \"\"\"Get an item from this node.\n",
      " \n",
      "         :param index: The node to use as a subscript index.\n",
      "-        :type index: Const or Slice\n",
      " \n",
      "         :raises AstroidTypeError: When the given index cannot be used as a\n",
      "             subscript index, or if this node is not subscriptable.\n",
      "         :raises AstroidIndexError: If the given index does not exist in the\n",
      "             dictionary.\n",
      "         \"\"\"\n",
      "+        # pylint: disable-next=import-outside-toplevel; circular import\n",
      "+        from astroid.helpers import safe_infer\n",
      "+\n",
      "         for key, value in self.items:\n",
      "             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n",
      "             if isinstance(key, DictUnpack):\n",
      "+                inferred_value = safe_infer(value, context)\n",
      "+                if not isinstance(inferred_value, Dict):\n",
      "+                    continue\n",
      "+\n",
      "                 try:\n",
      "-                    return value.getitem(index, context)\n",
      "+                    return inferred_value.getitem(index, context)\n",
      "                 except (AstroidTypeError, AstroidIndexError):\n",
      "                     continue\n",
      "+\n",
      "             for inferredkey in key.infer(context):\n",
      "                 if inferredkey is util.Uninferable:\n",
      "                     continue\n",
      "\n",
      "Instance ID: pylint-dev__astroid-1866.\n",
      "Patch:\n",
      "diff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py\n",
      "--- a/astroid/brain/brain_builtin_inference.py\n",
      "+++ b/astroid/brain/brain_builtin_inference.py\n",
      "@@ -954,8 +954,10 @@ def _infer_str_format_call(\n",
      " \n",
      "     try:\n",
      "         formatted_string = format_template.format(*pos_values, **keyword_values)\n",
      "-    except (IndexError, KeyError):\n",
      "-        # If there is an IndexError there are too few arguments to interpolate\n",
      "+    except (IndexError, KeyError, TypeError, ValueError):\n",
      "+        # IndexError: there are too few arguments to interpolate\n",
      "+        # TypeError: Unsupported format string\n",
      "+        # ValueError: Unknown format code\n",
      "         return iter([util.Uninferable])\n",
      " \n",
      "     return iter([nodes.const_factory(formatted_string)])\n",
      "\n",
      "Instance ID: pyvista__pyvista-4315.\n",
      "Patch:\n",
      "diff --git a/pyvista/core/grid.py b/pyvista/core/grid.py\n",
      "--- a/pyvista/core/grid.py\n",
      "+++ b/pyvista/core/grid.py\n",
      "@@ -135,23 +135,30 @@ def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n",
      "                     self.shallow_copy(args[0])\n",
      "             elif isinstance(args[0], (str, pathlib.Path)):\n",
      "                 self._from_file(args[0], **kwargs)\n",
      "-            elif isinstance(args[0], np.ndarray):\n",
      "-                self._from_arrays(args[0], None, None, check_duplicates)\n",
      "+            elif isinstance(args[0], (np.ndarray, Sequence)):\n",
      "+                self._from_arrays(np.asanyarray(args[0]), None, None, check_duplicates)\n",
      "             else:\n",
      "                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n",
      " \n",
      "         elif len(args) == 3 or len(args) == 2:\n",
      "-            arg0_is_arr = isinstance(args[0], np.ndarray)\n",
      "-            arg1_is_arr = isinstance(args[1], np.ndarray)\n",
      "+            arg0_is_arr = isinstance(args[0], (np.ndarray, Sequence))\n",
      "+            arg1_is_arr = isinstance(args[1], (np.ndarray, Sequence))\n",
      "             if len(args) == 3:\n",
      "-                arg2_is_arr = isinstance(args[2], np.ndarray)\n",
      "+                arg2_is_arr = isinstance(args[2], (np.ndarray, Sequence))\n",
      "             else:\n",
      "                 arg2_is_arr = False\n",
      " \n",
      "             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n",
      "-                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n",
      "+                self._from_arrays(\n",
      "+                    np.asanyarray(args[0]),\n",
      "+                    np.asanyarray(args[1]),\n",
      "+                    np.asanyarray(args[2]),\n",
      "+                    check_duplicates,\n",
      "+                )\n",
      "             elif all([arg0_is_arr, arg1_is_arr]):\n",
      "-                self._from_arrays(args[0], args[1], None, check_duplicates)\n",
      "+                self._from_arrays(\n",
      "+                    np.asanyarray(args[0]), np.asanyarray(args[1]), None, check_duplicates\n",
      "+                )\n",
      "             else:\n",
      "                 raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n",
      " \n",
      "\n",
      "Instance ID: pydicom__pydicom-1694.\n",
      "Patch:\n",
      "diff --git a/pydicom/dataset.py b/pydicom/dataset.py\n",
      "--- a/pydicom/dataset.py\n",
      "+++ b/pydicom/dataset.py\n",
      "@@ -2492,8 +2492,8 @@ def to_json_dict(\n",
      "         json_dataset = {}\n",
      "         for key in self.keys():\n",
      "             json_key = '{:08X}'.format(key)\n",
      "-            data_element = self[key]\n",
      "             try:\n",
      "+                data_element = self[key]\n",
      "                 json_dataset[json_key] = data_element.to_json_dict(\n",
      "                     bulk_data_element_handler=bulk_data_element_handler,\n",
      "                     bulk_data_threshold=bulk_data_threshold\n",
      "\n",
      "Instance ID: pydicom__pydicom-1413.\n",
      "Patch:\n",
      "diff --git a/pydicom/dataelem.py b/pydicom/dataelem.py\n",
      "--- a/pydicom/dataelem.py\n",
      "+++ b/pydicom/dataelem.py\n",
      "@@ -433,13 +433,24 @@ def value(self) -> Any:\n",
      "     @value.setter\n",
      "     def value(self, val: Any) -> None:\n",
      "         \"\"\"Convert (if necessary) and set the value of the element.\"\"\"\n",
      "+        # Ignore backslash characters in these VRs, based on:\n",
      "+        # * Which str VRs can have backslashes in Part 5, Section 6.2\n",
      "+        # * All byte VRs\n",
      "+        exclusions = [\n",
      "+            'LT', 'OB', 'OD', 'OF', 'OL', 'OV', 'OW', 'ST', 'UN', 'UT',\n",
      "+            'OB/OW', 'OW/OB', 'OB or OW', 'OW or OB',\n",
      "+            # Probably not needed\n",
      "+            'AT', 'FD', 'FL', 'SQ', 'SS', 'SL', 'UL',\n",
      "+        ]\n",
      "+\n",
      "         # Check if is a string with multiple values separated by '\\'\n",
      "         # If so, turn them into a list of separate strings\n",
      "         #  Last condition covers 'US or SS' etc\n",
      "-        if isinstance(val, (str, bytes)) and self.VR not in \\\n",
      "-                ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL',\n",
      "-                 'SQ', 'SS', 'UL', 'OB/OW', 'OW/OB', 'OB or OW',\n",
      "-                 'OW or OB', 'UN'] and 'US' not in self.VR:\n",
      "+        if (\n",
      "+            isinstance(val, (str, bytes))\n",
      "+            and self.VR not in exclusions\n",
      "+            and 'US' not in self.VR\n",
      "+        ):\n",
      "             try:\n",
      "                 if _backslash_str in val:\n",
      "                     val = cast(str, val).split(_backslash_str)\n",
      "\n",
      "Instance ID: pydicom__pydicom-1256.\n",
      "Patch:\n",
      "diff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n",
      "--- a/pydicom/jsonrep.py\n",
      "+++ b/pydicom/jsonrep.py\n",
      "@@ -226,7 +226,8 @@ def get_sequence_item(self, value):\n",
      "                     value_key = unique_value_keys[0]\n",
      "                     elem = DataElement.from_json(\n",
      "                         self.dataset_class, key, vr,\n",
      "-                        val[value_key], value_key\n",
      "+                        val[value_key], value_key,\n",
      "+                        self.bulk_data_element_handler\n",
      "                     )\n",
      "                 ds.add(elem)\n",
      "         return ds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sqlfluff__sqlfluff-1625',\n",
       " 'sqlfluff__sqlfluff-2419',\n",
       " 'sqlfluff__sqlfluff-1733',\n",
       " 'marshmallow-code__marshmallow-1359',\n",
       " 'pvlib__pvlib-python-1072',\n",
       " 'pvlib__pvlib-python-1154',\n",
       " 'pylint-dev__astroid-1196',\n",
       " 'pylint-dev__astroid-1866',\n",
       " 'pyvista__pyvista-4315',\n",
       " 'pydicom__pydicom-1694',\n",
       " 'pydicom__pydicom-1413',\n",
       " 'pydicom__pydicom-1256']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any existing patch file of the form essai-<instance_id>.patch\n",
    "for f in os.listdir(working_stage):\n",
    "    if f.startswith(f\"essai\") and f.endswith(\".patch\"):\n",
    "        os.remove(f\"{working_stage}/{f}\")\n",
    "# Find a patch with a specific number of edits\n",
    "num_files = 1\n",
    "lo_edits = 1\n",
    "hi_edits = 1\n",
    "lo_lines = 1\n",
    "hi_lines = 1000\n",
    "matches = (df['num_edits'] >= lo_edits) &\\\n",
    "    (df['num_edits'] <= hi_edits) &\\\n",
    "    (df['num_files'] == num_files) &\\\n",
    "    (df['num_lines'] >= lo_lines) &\\\n",
    "    (df['num_lines'] <= hi_lines)\n",
    "# specific_instance_id = \"sqlfluff__sqlfluff-1763\"\n",
    "# matches = df['instance_ids'] == specific_instance_id\n",
    "df_matches = df[matches]\n",
    "retriever = GoldenRetriever(dataset_name=dataset, split=split, num_shards=None, shard_id=None)\n",
    "print(f\"Found {len(df_matches)}.\")\n",
    "print(df_matches.head())\n",
    "res = []\n",
    "for instance_id in df_matches['instance_ids']:\n",
    "    patch = retriever.gold_patches[instance_id]\n",
    "    print(f\"Instance ID: {instance_id}.\")\n",
    "    print(f\"Patch:\\n{patch}\")\n",
    "    patch_file = f\"{working_stage}/essai-{instance_id}.patch\"\n",
    "    with open(patch_file, 'w') as f:\n",
    "        f.write(f\"Instance ID: {instance_id}\\n\")\n",
    "        f.write(f\"{patch}\")\n",
    "    res.append(instance_id)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
